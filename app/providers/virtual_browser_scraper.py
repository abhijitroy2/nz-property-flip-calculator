import asyncio
import re
from typing import Optional, Dict, Any
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from .base import Provider
from .. import models

class VirtualBrowserScraper(Provider):
    """Virtual browser scraper using Selenium Chrome WebDriver"""
    
    def __init__(self):
        self.driver = None
        self.rate_limit_delay = 10  # 10 seconds between requests
        self.last_request_time = 0
        self._setup_driver()
    
    def _setup_driver(self):
        """Setup Chrome WebDriver with optimal settings"""
        try:
            chrome_options = Options()
            
            # Add stealth options to avoid detection
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # Set realistic user agent
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # Optional: Run headless (comment out to see browser)
            # chrome_options.add_argument("--headless")
            
            # Setup ChromeDriver service
            service = Service(ChromeDriverManager().install())
            
            # Create driver
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # Execute script to remove webdriver property
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Virtual browser (Chrome WebDriver) initialized successfully")
            
        except Exception as e:
            print(f"âŒ Failed to initialize virtual browser: {e}")
            self.driver = None
    
    async def fetch(self, address: str, url: Optional[str] = None, site_name: str = "Unknown") -> models.DataPoints:
        """Fetch property data using virtual browser"""
        try:
            if url and self.driver:
                # Scrape from specific URL using virtual browser
                valuation_data = await self._scrape_with_browser(url, site_name)
            else:
                # Fall back to estimated data
                valuation_data = self._estimate_valuation_data(address)
            
            return models.DataPoints(
                address=address,
                current_valuation_low=valuation_data.get('low'),
                current_valuation_mid=valuation_data.get('mid'),
                current_valuation_high=valuation_data.get('high'),
                last_sale_price=valuation_data.get('last_sale_price'),
                last_sale_date=valuation_data.get('last_sale_date'),
                valuation_source=valuation_data.get('source', f'{site_name} (Virtual Browser)'),
                method_of_sale=valuation_data.get('method_of_sale'),
            )
        except Exception as e:
            print(f"Virtual Browser Scraper error: {e}")
            # Fallback to estimated data
            if url:
                valuation_data = self._estimate_valuation_data_from_url(url, site_name)
            else:
                valuation_data = self._estimate_valuation_data(address)
            
            return models.DataPoints(
                address=address,
                current_valuation_low=valuation_data.get('low'),
                current_valuation_mid=valuation_data.get('mid'),
                current_valuation_high=valuation_data.get('high'),
                last_sale_price=valuation_data.get('last_sale_price'),
                last_sale_date=valuation_data.get('last_sale_date'),
                valuation_source=valuation_data.get('source', 'Estimated'),
                method_of_sale=valuation_data.get('method_of_sale'),
            )
    
    async def _scrape_with_browser(self, url: str, site_name: str) -> Dict[str, Any]:
        """Scrape property data using virtual browser"""
        try:
            # Rate limiting
            await self._enforce_rate_limit()
            
            if not self.driver:
                raise Exception("Virtual browser not initialized")
            
            # Navigate to URL
            print(f"ðŸŒ Virtual browser navigating to: {url}")
            self.driver.get(url)
            
            # Wait for initial page load
            await asyncio.sleep(3)
            
            # Gradual scroll to bottom of page to load all dynamic content
            print("ðŸ“œ Gradually scrolling down to load all content...")
            scroll_pause_time = 1
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            while True:
                # Scroll down gradually (like a human would)
                current_position = self.driver.execute_script("return window.pageYOffset")
                scroll_increment = 500  # Scroll 500px at a time
                new_position = current_position + scroll_increment
                
                self.driver.execute_script(f"window.scrollTo(0, {new_position});")
                await asyncio.sleep(scroll_pause_time)
                
                # Check if we've reached the bottom
                current_position = self.driver.execute_script("return window.pageYOffset")
                total_height = self.driver.execute_script("return document.body.scrollHeight")
                
                if current_position + 1000 >= total_height:  # Near bottom
                    # Try one more scroll to trigger any remaining content
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    await asyncio.sleep(scroll_pause_time)
                    
                    # Check if new content loaded
                    new_height = self.driver.execute_script("return document.body.scrollHeight")
                    if new_height == last_height:
                        break
                    last_height = new_height
                else:
                    # Continue scrolling
                    continue
            
            # Scroll back to top gradually
            print("ðŸ“œ Gradually scrolling back to top...")
            current_position = self.driver.execute_script("return window.pageYOffset")
            while current_position > 0:
                current_position = max(0, current_position - 500)
                self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                await asyncio.sleep(0.5)
            
            await asyncio.sleep(1)
            
            # Get page source
            page_source = self.driver.page_source
            
            # Parse the page content
            property_data = self._parse_page_content(page_source, url, site_name)
            
            return property_data
            
        except Exception as e:
            print(f"Error scraping with virtual browser: {e}")
            return self._estimate_valuation_data_from_url(url, site_name)
    
    def _parse_page_content(self, html_content: str, url: str, site_name: str) -> Dict[str, Any]:
        """Parse property data from page HTML"""
        try:
            # Extract method of sale from specific TradeMe element
            method_of_sale = self._extract_method_of_sale(html_content)
            
            # Extract valuation range from specific TradeMe element
            valuation_range = self._extract_valuation_range(html_content)
            
            # If we found a valuation range, use it
            if valuation_range:
                low_price, high_price = valuation_range
                mid_price = (low_price + high_price) / 2
            else:
                # Fallback to general price extraction patterns
                price_patterns = [
                    r'<span[^>]*class="[^"]*price[^"]*"[^>]*>\$?([\d,]+)',
                    r'<div[^>]*class="[^"]*price[^"]*"[^>]*>\$?([\d,]+)',
                    r'Price[:\s]*\$?([\d,]+)',
                    r'\$([\d,]+)',
                    r'<h[1-6][^>]*>\$?([\d,]+)',
                    r'<strong[^>]*>\$?([\d,]+)',
                    r'<span[^>]*>\$?([\d,]+)',
                    r'<p[^>]*>\$?([\d,]+)',
                    r'data-price="([\d,]+)"',
                    r'price["\s]*:["\s]*([\d,]+)'
                ]
                
                mid_price = None
                for pattern in price_patterns:
                    matches = re.findall(pattern, html_content, re.IGNORECASE)
                    for match in matches:
                        price_str = match.replace(',', '')
                        if price_str.isdigit():
                            mid_price = float(price_str)
                            if mid_price > 100000:  # Reasonable property price
                                break
                    if mid_price:
                        break
                
                if mid_price:
                    low_price = mid_price * 0.85
                    high_price = mid_price * 1.15
                else:
                    low_price = high_price = mid_price = None
            
            # Extract property details
            bedrooms = self._extract_number(html_content, r'(\d+)\s*bed')
            bathrooms = self._extract_number(html_content, r'(\d+)\s*bath')
            area = self._extract_number(html_content, r'(\d+)\s*mÂ²')
            
            # Extract listing date
            date_patterns = [
                r'Listed[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
                r'Date[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
                r'(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})'
            ]
            
            listing_date = None
            for pattern in date_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    listing_date = match.group(1)
                    break
            
            if mid_price:
                return {
                    'low': round(low_price, 0),
                    'mid': round(mid_price, 0),
                    'high': round(high_price, 0),
                    'last_sale_price': round(mid_price, 0),
                    'last_sale_date': listing_date,
                    'source': f'{site_name} (Virtual Browser - Live)',
                    'method_of_sale': method_of_sale,
                    'bedrooms': bedrooms,
                    'bathrooms': bathrooms,
                    'area': area
                }
            else:
                # If we couldn't extract price, fall back to estimation but preserve method of sale
                estimated_data = self._estimate_valuation_data_from_url(url, site_name)
                estimated_data['method_of_sale'] = method_of_sale
                return estimated_data
                
        except Exception as e:
            print(f"Error parsing page content: {e}")
            # Try to extract method of sale even if other parsing fails
            try:
                method_of_sale = self._extract_method_of_sale(html_content)
            except:
                method_of_sale = None
            estimated_data = self._estimate_valuation_data_from_url(url, site_name)
            estimated_data['method_of_sale'] = method_of_sale
            return estimated_data
    
    def _extract_number(self, html_content: str, pattern: str) -> Optional[int]:
        """Extract a number from HTML using regex pattern"""
        match = re.search(pattern, html_content, re.IGNORECASE)
        if match:
            try:
                return int(match.group(1))
            except ValueError:
                pass
        return None
    
    def _extract_method_of_sale(self, html_content: str) -> Optional[str]:
        """Extract method of sale from TradeMe specific element"""
        try:
            # Look for the specific TradeMe method of sale element
            # <strong _ngcontent-frend-c1893091350="">To be sold by auction on 7 Nov, 10:00 am</strong>
            method_patterns = [
                # Most specific pattern for TradeMe strong tags with auction info
                r'<strong[^>]*>\s*([^<]*sold by auction[^<]*)\s*</strong>',
                r'<strong[^>]*>\s*([^<]*To be sold by[^<]*)\s*</strong>',
                r'<strong[^>]*>\s*([^<]*auction[^<]*)\s*</strong>',
                r'<strong[^>]*>\s*([^<]*tender[^<]*)\s*</strong>',
                r'<strong[^>]*>\s*([^<]*negotiation[^<]*)\s*</strong>',
                r'<strong[^>]*>\s*([^<]*price[^<]*)\s*</strong>',
                # Look for auction info in description text
                r'Auction[:\s]*([^<\n]+)',
                r'To be sold by ([^<\n]+)',
                r'sold by ([^<\n]+)',
                r'Method[:\s]*([^<\n]+)',
                r'Sale[:\s]*([^<\n]+)',
                # Look for any strong tag with sale-related content
                r'<strong[^>]*>\s*([^<]*(?:sale|auction|tender|negotiation|price)[^<]*)\s*</strong>',
                # Look for auction info in paragraph text
                r'<p[^>]*>\s*([^<]*Auction[^<]*)\s*</p>',
                r'<p[^>]*>\s*([^<]*auction[^<]*)\s*</p>'
            ]
            
            print(f"ðŸ” Searching for method of sale in HTML content...")
            
            for i, pattern in enumerate(method_patterns):
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                print(f"Pattern {i+1}: Found {len(matches)} matches")
                
                for j, method in enumerate(matches):
                    method = method.strip()
                    print(f"  Match {j+1}: '{method}'")
                    if method and len(method) > 5:  # Valid method found
                        print(f"âœ… Valid method of sale found: {method}")
                        return method
            
            print("âŒ No valid method of sale found")
            return None
        except Exception as e:
            print(f"Error extracting method of sale: {e}")
            return None
    
    def _extract_valuation_range(self, html_content: str) -> Optional[tuple]:
        """Extract valuation range from TradeMe specific element"""
        try:
            # Save HTML content for inspection
            with open("debug_trademe_html.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            print("ðŸ’¾ Saved HTML content to debug_trademe_html.html for inspection")
            
            # Look for the specific TradeMe valuation range element
            # <p _ngcontent-frend-c38026251="" class="p-h1">$840K - $945K</p>
            valuation_patterns = [
                # Most specific pattern for TradeMe p-h1 class
                r'<p[^>]*class="[^"]*p-h1[^"]*"[^>]*>\s*\$?([\d,]+[KMB]?)\s*-\s*\$?([\d,]+[KMB]?)\s*</p>',
                # Pattern for any p tag with price range
                r'<p[^>]*>\s*\$?([\d,]+[KMB]?)\s*-\s*\$?([\d,]+[KMB]?)\s*</p>',
                # Pattern for span with price range
                r'<span[^>]*>\s*\$?([\d,]+[KMB]?)\s*-\s*\$?([\d,]+[KMB]?)\s*</span>',
                # Pattern for div with price range
                r'<div[^>]*>\s*\$?([\d,]+[KMB]?)\s*-\s*\$?([\d,]+[KMB]?)\s*</div>',
                # Pattern for estimate text
                r'Estimate[:\s]*\$?([\d,]+[KMB]?)\s*-\s*\$?([\d,]+[KMB]?)',
                # Pattern for valuation text
                r'Valuation[:\s]*\$?([\d,]+[KMB]?)\s*-\s*\$?([\d,]+[KMB]?)',
                # Pattern for price range with K/M/B suffixes
                r'\$?([\d,]+[KMB])\s*-\s*\$?([\d,]+[KMB])'
            ]
            
            print(f"ðŸ” Searching for valuation range in HTML content...")
            
            for i, pattern in enumerate(valuation_patterns):
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                print(f"Pattern {i+1}: Found {len(matches)} matches")
                
                for j, match in enumerate(matches):
                    low_str, high_str = match
                    print(f"  Match {j+1}: '{low_str}' - '{high_str}'")
                    
                    # Convert K/M/B suffixes to numbers
                    low_price = self._convert_price_string(low_str)
                    high_price = self._convert_price_string(high_str)
                    
                    print(f"  Converted: {low_price} - {high_price}")
                    
                    # More strict validation - must be reasonable property prices
                    if (low_price and high_price and 
                        low_price >= 100000 and high_price >= 100000 and 
                        high_price > low_price and 
                        high_price <= 10000000):  # Max $10M
                        print(f"âœ… Valid valuation range found: ${low_price:,.0f} - ${high_price:,.0f}")
                        return (low_price, high_price)
            
            print("âŒ No valid valuation range found")
            return None
        except Exception as e:
            print(f"Error extracting valuation range: {e}")
            return None
    
    def _convert_price_string(self, price_str: str) -> Optional[float]:
        """Convert price string with K/M/B suffixes to float"""
        try:
            price_str = price_str.strip().replace(',', '').replace('$', '')
            
            if price_str.endswith('K'):
                return float(price_str[:-1]) * 1000
            elif price_str.endswith('M'):
                return float(price_str[:-1]) * 1000000
            elif price_str.endswith('B'):
                return float(price_str[:-1]) * 1000000000
            else:
                return float(price_str)
        except (ValueError, AttributeError):
            return None
    
    async def _enforce_rate_limit(self):
        """Enforce rate limiting between requests"""
        current_time = asyncio.get_event_loop().time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.rate_limit_delay:
            wait_time = self.rate_limit_delay - time_since_last
            print(f"Virtual browser rate limiting: waiting {wait_time:.1f} seconds...")
            await asyncio.sleep(wait_time)
        
        self.last_request_time = asyncio.get_event_loop().time()
    
    def _estimate_valuation_data_from_url(self, url: str, site_name: str) -> Dict[str, Any]:
        """Estimate valuation data based on URL characteristics"""
        # Extract more detailed location info from URL
        url_lower = url.lower()
        
        # Extract city/region
        city = 'auckland'  # default
        if 'auckland' in url_lower:
            city = 'auckland'
        elif 'wellington' in url_lower:
            city = 'wellington'
        elif 'christchurch' in url_lower:
            city = 'christchurch'
        elif 'hamilton' in url_lower:
            city = 'hamilton'
        elif 'tauranga' in url_lower:
            city = 'tauranga'
        elif 'dunedin' in url_lower:
            city = 'dunedin'
        
        # Extract property type from URL
        property_type = 'residential'
        if 'commercial' in url_lower:
            property_type = 'commercial'
        elif 'land' in url_lower:
            property_type = 'land'
        elif 'apartment' in url_lower:
            property_type = 'apartment'
        elif 'new-homes' in url_lower:
            property_type = 'new_home'
        
        # Extract suburb if available
        suburb = None
        url_parts = url_lower.split('/')
        for i, part in enumerate(url_parts):
            if part in ['auckland', 'wellington', 'christchurch', 'hamilton', 'tauranga', 'dunedin']:
                if i + 1 < len(url_parts):
                    suburb = url_parts[i + 1]
                break
        
        # Get base price and adjust for property type and location
        base_price = self._get_base_price_for_city(city)
        
        # Adjust for property type
        if property_type == 'commercial':
            base_price *= 0.8  # Commercial properties vary widely
        elif property_type == 'land':
            base_price *= 0.6  # Land typically costs less
        elif property_type == 'apartment':
            base_price *= 0.9  # Apartments typically cost less than houses
        elif property_type == 'new_home':
            base_price *= 1.1  # New homes typically cost more
        
        # Adjust for premium suburbs
        if suburb and suburb in ['remuera', 'ponsonby', 'takapuna', 'epsom', 'herne bay']:
            base_price *= 1.3  # Premium Auckland suburbs
        elif suburb and suburb in ['thorndon', 'kelburn', 'wadestown']:
            base_price *= 1.2  # Premium Wellington suburbs
        
        # Generate realistic variation
        import hashlib
        import random
        from datetime import datetime, timedelta
        
        url_hash = hashlib.md5(url.encode()).hexdigest()
        rng = random.Random(int(url_hash[:8], 16))
        
        # Add variation based on URL
        variation = rng.uniform(0.85, 1.15)
        mid_price = base_price * variation
        
        # Calculate low and high
        low_price = mid_price * rng.uniform(0.80, 0.90)
        high_price = mid_price * rng.uniform(1.10, 1.20)
        
        # Generate last sale data
        last_sale_price = mid_price * rng.uniform(0.75, 1.05)
        days_ago = rng.randint(60, 1095)  # 2 months to 3 years ago
        last_sale_date = (datetime.now() - timedelta(days=days_ago)).strftime("%d/%m/%Y")
        
        return {
            'low': round(low_price, 0),
            'mid': round(mid_price, 0),
            'high': round(high_price, 0),
            'last_sale_price': round(last_sale_price, 0),
            'last_sale_date': last_sale_date,
            'source': f'{site_name} URL Analysis (Virtual Browser blocked)',
            'property_type': property_type,
            'suburb': suburb
        }
    
    def _estimate_valuation_data(self, address: str) -> Dict[str, Any]:
        """Estimate valuation data based on address characteristics"""
        import hashlib
        import random
        from datetime import datetime, timedelta
        
        address_hash = hashlib.md5(address.encode()).hexdigest()
        rng = random.Random(address_hash)
        
        # Get base price based on city
        base_price = self._get_base_price_for_city(address)
        
        # Add some variation
        variation = rng.uniform(0.8, 1.2)
        mid_price = base_price * variation
        
        # Calculate low and high (typically Â±15-25%)
        low_price = mid_price * rng.uniform(0.75, 0.85)
        high_price = mid_price * rng.uniform(1.15, 1.25)
        
        # Generate last sale data
        last_sale_price = mid_price * rng.uniform(0.7, 1.1)
        days_ago = rng.randint(30, 1825)  # 1 month to 5 years ago
        last_sale_date = (datetime.now() - timedelta(days=days_ago)).strftime("%d/%m/%Y")
        
        return {
            'low': round(low_price, 0),
            'mid': round(mid_price, 0),
            'high': round(high_price, 0),
            'last_sale_price': round(last_sale_price, 0),
            'last_sale_date': last_sale_date,
            'source': 'Estimated (Virtual Browser unavailable)'
        }
    
    def _get_base_price_for_city(self, address: str) -> float:
        """Get base price for city based on address"""
        address_lower = address.lower()
        
        # Base prices for major NZ cities (in NZD)
        city_prices = {
            'auckland': 1200000,  # High prices
            'wellington': 950000,  # High prices
            'christchurch': 650000,  # Moderate prices
            'hamilton': 580000,  # Moderate prices
            'tauranga': 720000,  # Moderate-high prices
            'dunedin': 520000,  # Lower prices
            'palmerston north': 480000,  # Lower prices
            'nelson': 680000,  # Moderate-high prices
            'rotorua': 450000,  # Lower prices
            'new plymouth': 500000,  # Moderate prices
            'remuera': 1800000,  # Very high prices
            'ponsonby': 1600000,  # Very high prices
            'takapuna': 1700000,  # Very high prices
            'epsom': 1750000,  # Very high prices
        }
        
        # Find matching city/suburb
        for city, price in city_prices.items():
            if city in address_lower:
                return price
        
        # Default to average NZ price
        return 650000
    
    async def close(self):
        """Close the virtual browser"""
        if self.driver:
            try:
                self.driver.quit()
                print("âœ… Virtual browser closed successfully")
            except Exception as e:
                print(f"Error closing virtual browser: {e}")
            finally:
                self.driver = None
